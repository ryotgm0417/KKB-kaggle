<<<<<<< HEAD
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"submission.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vGJGsJfsggSr","colab_type":"code","outputId":"b505edd3-97d3-475d-fed3-5a23b58bf9fb","executionInfo":{"status":"ok","timestamp":1583990038469,"user_tz":-540,"elapsed":911,"user":{"displayName":"bumjun jung","photoUrl":"","userId":"01001136615869141619"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","%cd gdrive/My\\ Drive/KKB-kaggle/bengaliai-cv19/notebooks"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n","/content/gdrive/My Drive/KKB-kaggle/bengaliai-cv19/notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U67CYVMTg7tE","colab_type":"code","colab":{}},"source":["　# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# Any results you write to the current directory are saved as output."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yNVEBroeewc4","colab_type":"code","outputId":"ddcd1cfe-b33a-4eab-cbe2-650d7bcabdd8","executionInfo":{"status":"ok","timestamp":1583990043076,"user_tz":-540,"elapsed":5509,"user":{"displayName":"bumjun jung","photoUrl":"","userId":"01001136615869141619"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!pip install efficientnet_pytorch"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.6/dist-packages (0.6.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.4.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TqMWjaRU2KU4","colab_type":"code","colab":{}},"source":["## 諸々の import\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import recall_score\n","import cv2\n","# from tqdm.auto import tqdm\n","import copy\n","import datetime\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms, utils\n","import torchvision.models as models\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import torchvision\n","import PIL\n","# from torchsummary import summary\n","import gc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RbsZSbkg9SrB","colab_type":"code","colab":{}},"source":["## Parameters\n","\n","# resize後のサイズ\n","HEIGHT = 64\n","WIDTH = 64\n","\n","# 画像を3次元にするかどうか（EfficientNetなどを使うときはTrue）\n","enable_3d = True\n","\n","# True なら Cross Validation を実施する\n","# Kaggle に提出するデータを作るときは False にしてください\n","do_validation = False\n","\n","# True なら submission.csv を生成する\n","create_submission = False\n","\n","val_perc = 0.1  # validation set の割合（クロスバリデーション）\n","\n","#epochs number\n","epochs = 20\n","\n","#初期値として、保存した重みを使う時はこれをTrueに\n","load_flag = True\n","\n","#Kaggleで提出するときはTrueにする\n","kaggle_flag = False\n","#loadするファイルへのパス\n","if kaggle_flag:\n","    model_path = '/kaggle/input/model_load/resnet18_epoch1_2020-03-09_14-58-43.pth'\n","else:\n","    model_path = '../trained_models/efficientnet_epoch52_2020-03-12_06-30-12.pth'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f2uk6pfBBxdy","colab_type":"code","colab":{}},"source":["if kaggle_flag:\n","    dataset_dir = '/kaggle/input/bengaliai-cv19'\n","    model_dir = '/kaggle/input/trained_models'\n","else:\n","    dataset_dir = '../dataset'\n","    model_dir = '../trained_models'\n","\n","train_df = pd.read_csv(dataset_dir + '/train.csv')\n","test_df = pd.read_csv(dataset_dir + '/test.csv')\n","class_map_df = pd.read_csv(dataset_dir + '/class_map.csv')\n","sample_sub_df = pd.read_csv(dataset_dir + '/sample_submission.csv')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RU66nFHqgj7K","colab_type":"code","outputId":"d51e3f6d-e44d-4d59-94c4-9bdb23f63291","executionInfo":{"status":"ok","timestamp":1583990053818,"user_tz":-540,"elapsed":3402,"user":{"displayName":"bumjun jung","photoUrl":"","userId":"01001136615869141619"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#モデルの設定\n","\n","from preprocess import *\n","from save_load import *\n","#from model.CNN import model\n","#from model.efficientnet import model\n","from model.efficientnetb1 import model\n","#from model.resnet18 import model\n","#from model.resnet34 import model\n","#from model.resnet50 import model\n","#from model.resnet101 import model\n","#from model.resnet152 import model\n","\n","model = model()\n","model = try_gpu(model)\n","optimizer = optim.Adam(model.parameters())\n","if load_flag:\n","    model,optimizer,start_epoch = load_model(model,optimizer,model_path)\n","    model = try_gpu(model)\n","    start_epoch += 1\n","else:\n","    start_epoch = 1    \n","criterion1 = nn.CrossEntropyLoss() \n","# optimizer = optim.Adam(model.parameters(), lr=0.0001) # epoch 23 までは0.001\n","optimizer = optim.Adam(model.parameters(), lr=0.00001) # epoch 52まで0.0001"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Loaded pretrained weights for efficientnet-b1\n","loaded from ../trained_models/efficientnet_epoch52_2020-03-12_06-30-12.pth\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h7G-i0XLtsbY","colab_type":"code","colab":{}},"source":["## train関数、test関数\n","\n","def train(model, epoch, train_loader):\n","    model.train()\n","    \n","    size = len(train_loader.dataset)\n","    pred_r, pred_v, pred_c = np.zeros(size), np.zeros(size), np.zeros(size)\n","    true_r, true_v, true_c = np.zeros(size), np.zeros(size), np.zeros(size)\n","    index = 0\n","\n","    for data in train_loader:\n","        inputs, root_l, vowel_l, consonant_l = data\n","        inputs, root_l, vowel_l, consonant_l = Variable(inputs), Variable(root_l), Variable(vowel_l), Variable(consonant_l)\n","        inputs, root_l, vowel_l, consonant_l = try_gpu(inputs), try_gpu(root_l), try_gpu(vowel_l), try_gpu(consonant_l)\n","        optimizer.zero_grad()\n","        root_o, vowel_o, consonant_o = model(inputs)\n","        root_pred, vowel_pred, consonant_pred = torch.max(root_o.data,1)[1], torch.max(vowel_o.data,1)[1], torch.max(consonant_o.data,1)[1]\n","        loss1 = criterion1(root_o, root_l)\n","        loss2 = criterion1(vowel_o, vowel_l)\n","        loss3 = criterion1(consonant_o, consonant_l)\n","        (loss1+loss2+loss3).backward()\n","        optimizer.step()\n","        for i in range(inputs.size(0)):\n","            pred_r[index] = root_pred[i]\n","            pred_v[index] = vowel_pred[i]\n","            pred_c[index] = consonant_pred[i]\n","            true_r[index] = root_l[i]\n","            true_v[index] = vowel_l[i]\n","            true_c[index] = consonant_l[i]\n","            index += 1\n","    recall_r = recall_score(true_r, pred_r, average='macro')\n","    recall_v = recall_score(true_v, pred_v, average='macro')\n","    recall_c = recall_score(true_c, pred_c, average='macro')\n","    final_score = (2.*recall_r + recall_v + recall_c) / 4.\n","\n","    print(f'Root Recall(train): {recall_r:.5f}')\n","    print(f'Vowel Recall(train): {recall_v:.5f}')\n","    print(f'Consonant Recall(train): {recall_c:.5f}')\n","    print(f'Score(train): {final_score:.5f}')\n","   \n","\n","def test(model, test_loader):\n","    model.eval()\n","\n","    size = len(test_loader.dataset)\n","    pred_r, pred_v, pred_c = np.zeros(size), np.zeros(size), np.zeros(size)\n","    true_r, true_v, true_c = np.zeros(size), np.zeros(size), np.zeros(size)\n","    index = 0\n","    \n","    for data in test_loader:\n","        inputs, root_l, vowel_l, consonant_l = data\n","        inputs, root_l, vowel_l, consonant_l = Variable(inputs), Variable(root_l), Variable(vowel_l), Variable(consonant_l)\n","        inputs, root_l, vowel_l, consonant_l = try_gpu(inputs), try_gpu(root_l), try_gpu(vowel_l), try_gpu(consonant_l)\n","        \n","        root_o, vowel_o, consonant_o = model(inputs) \n","        root_pred, vowel_pred, consonant_pred = torch.max(root_o.data,1)[1], torch.max(vowel_o.data,1)[1], torch.max(consonant_o.data,1)[1]\n","        for i in range(inputs.size(0)):\n","            pred_r[index] = root_pred[i]\n","            pred_v[index] = vowel_pred[i]\n","            pred_c[index] = consonant_pred[i]\n","            true_r[index] = root_l[i]\n","            true_v[index] = vowel_l[i]\n","            true_c[index] = consonant_l[i]\n","            index += 1\n","\n","    recall_r = recall_score(true_r, pred_r, average='macro')\n","    recall_v = recall_score(true_v, pred_v, average='macro')\n","    recall_c = recall_score(true_c, pred_c, average='macro')\n","    final_score = (2.*recall_r + recall_v + recall_c) / 4.\n","\n","    print(f'Root Recall(test): {recall_r:.5f}')\n","    print(f'Vowel Recall(test): {recall_v:.5f}')\n","    print(f'Consonant Recall(test): {recall_c:.5f}')\n","    print(f'Score(test): {final_score:.5f}')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5wZ0lQFJ3Ajz","colab_type":"code","outputId":"c15cce12-d4c6-48fe-b4c3-d4b2efa125a8","executionInfo":{"status":"ok","timestamp":1583940844227,"user_tz":-540,"elapsed":362805,"user":{"displayName":"bumjun jung","photoUrl":"","userId":"01001136615869141619"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["## データの読み込み\n","\n","X_all = np.empty((0, HEIGHT*WIDTH))\n","Y_root_all = np.empty((0, 168))\n","Y_vowel_all = np.empty((0, 11))\n","Y_cons_all = np.empty((0, 7))\n","\n","for parq_i in range(4):\n","    print(f'Parquet {parq_i} を読み込み中')\n","    train_df_with_img = pd.merge(pd.read_parquet(dataset_dir + f'/train_image_data_{parq_i}.parquet'), train_df, on='image_id').drop(['image_id'], axis=1)\n","    \n","    X = train_df_with_img.drop(columns=['grapheme_root', 'vowel_diacritic', 'consonant_diacritic', 'grapheme'])\n","    X_resized = resize(X, out_height=HEIGHT, out_width=WIDTH).astype(np.uint8) # astype(np.uint8)をしてあげることで後で cv2.cvtColor(out_data, cv2.COLOR_GRAY2RGB) が実行できるようになる\n","    \n","    Y_root = pd.get_dummies(train_df_with_img['grapheme_root']).values\n","    Y_vowel = pd.get_dummies(train_df_with_img['vowel_diacritic']).values\n","    Y_cons = pd.get_dummies(train_df_with_img['consonant_diacritic']).values\n","\n","    X_all = np.append(X_all, X_resized, axis=0)\n","    Y_root_all = np.append(Y_root_all, Y_root, axis=0)\n","    Y_vowel_all = np.append(Y_vowel_all, Y_vowel, axis=0)\n","    Y_cons_all = np.append(Y_cons_all, Y_cons, axis=0)\n","\n","    del X\n","    del X_resized\n","    del Y_root\n","    del Y_vowel \n","    del Y_cons \n","    gc.collect()\n","\n","print(X_all.shape)\n","print(Y_root_all.shape)\n","print(Y_vowel_all.shape)\n","print(Y_cons_all.shape)\n","\n","Y_all = [Y_root_all, Y_vowel_all, Y_cons_all]\n","\n","trainval_dataset = MyDataset(X_all, Y_all, enable_3d=enable_3d, H=HEIGHT, W=WIDTH)\n","\n","del X_all\n","del Y_root_all\n","del Y_vowel_all\n","del Y_cons_all\n","del Y_all\n","gc.collect()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Parquet 0 を読み込み中\n","Resizing raw image... / 前処理実行中…\n","Parquet 1 を読み込み中\n","Resizing raw image... / 前処理実行中…\n","Parquet 2 を読み込み中\n","Resizing raw image... / 前処理実行中…\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ee2x8byvB8eW","colab_type":"code","outputId":"e01cfbfc-31af-4994-e413-d90a36a7a256","executionInfo":{"status":"error","timestamp":1583915243678,"user_tz":-540,"elapsed":132401,"user":{"displayName":"bumjun jung","photoUrl":"","userId":"01001136615869141619"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["## 訓練ループ / Training Loop Kaggleでやる際には使わない\n","\n","print('==========================')\n","print('Starting training.')\n","\n","if do_validation:\n","    n_samples = len(trainval_dataset)\n","    train_size = int(len(trainval_dataset)*(1.0 - val_perc))\n","    val_size = n_samples - train_size\n","    print(f'train size: {train_size}, validation size: {val_size}')\n","\n","    subset, val_dataset = torch.utils.data.random_split(trainval_dataset, [train_size, val_size])\n","\n","    if enable_3d:\n","        train_dataset = TransformDataset(subset, transform=transforms.RandomChoice(\n","            [transform_none, transform_crop224, transform_rotate, transform_noise]\n","        ))\n","\n","    else:\n","        train_dataset = TransformDataset(subset, transform=transforms.RandomChoice(\n","            [transform_none, transform_crop64, transform_rotate, transform_noise]\n","        ))\n","\n","    del trainval_dataset\n","    del subset\n","    gc.collect()\n","\n","    train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=4)\n","    val_loader = DataLoader(dataset=val_dataset, batch_size=32, num_workers=0)\n","\n","    for i in range(start_epoch,epochs+start_epoch):\n","        print(f'Epoch number {i}')\n","        train(model, i, train_loader)\n","        test(model, val_loader)\n","        save_model(model,optimizer,model_dir, i)\n","\n","    # メモリ節約\n","    del train_dataset\n","    del val_dataset\n","    del train_loader\n","    del val_loader\n","    gc.collect()\n","\n","else:\n","    n_samples = len(trainval_dataset)\n","    print(f'train size: {n_samples}')\n","\n","    if enable_3d:\n","        trainval_dataset.transform = transforms.RandomChoice(\n","            [transform_none, transform_crop224, transform_rotate, transform_noise]\n","        )\n","    \n","    else:\n","        trainval_dataset.transform = transforms.RandomChoice(\n","            [transform_none, transform_crop64, transform_rotate, transform_noise]\n","        )\n","\n","    train_loader = DataLoader(dataset=trainval_dataset, batch_size=32, shuffle=True, num_workers=4)\n","\n","    for i in range(start_epoch,epochs+start_epoch):\n","        print(f'Epoch number {i}')\n","        train(model, i, train_loader)\n","        save_model(model,optimizer,model_dir, i)\n","\n","    # メモリ節約\n","    del trainval_dataset\n","    del train_loader\n","    gc.collect()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["==========================\n","Starting training.\n","train size: 200840\n","Epoch number 38\n","Root Recall(train): 0.99215\n","Vowel Recall(train): 0.99603\n","Consonant Recall(train): 0.99346\n","Score(train): 0.99345\n","---saving model of epoch 38---\n","save finished\n","Epoch number 39\n","Root Recall(train): 0.99248\n","Vowel Recall(train): 0.99563\n","Consonant Recall(train): 0.99484\n","Score(train): 0.99386\n","---saving model of epoch 39---\n","save finished\n","Epoch number 40\n","Root Recall(train): 0.99260\n","Vowel Recall(train): 0.99606\n","Consonant Recall(train): 0.99522\n","Score(train): 0.99412\n","---saving model of epoch 40---\n","save finished\n","Epoch number 41\n","Root Recall(train): 0.99326\n","Vowel Recall(train): 0.99642\n","Consonant Recall(train): 0.99508\n","Score(train): 0.99451\n","---saving model of epoch 41---\n","save finished\n","Epoch number 42\n","Root Recall(train): 0.99295\n","Vowel Recall(train): 0.99589\n","Consonant Recall(train): 0.99608\n","Score(train): 0.99447\n","---saving model of epoch 42---\n","save finished\n","Epoch number 43\n","Root Recall(train): 0.99386\n","Vowel Recall(train): 0.99627\n","Consonant Recall(train): 0.99638\n","Score(train): 0.99509\n","---saving model of epoch 43---\n","save finished\n","Epoch number 44\n","Root Recall(train): 0.99373\n","Vowel Recall(train): 0.99639\n","Consonant Recall(train): 0.99619\n","Score(train): 0.99501\n","---saving model of epoch 44---\n","save finished\n","Epoch number 45\n","Root Recall(train): 0.99386\n","Vowel Recall(train): 0.99617\n","Consonant Recall(train): 0.99524\n","Score(train): 0.99478\n","---saving model of epoch 45---\n","save finished\n","Epoch number 46\n","Root Recall(train): 0.99395\n","Vowel Recall(train): 0.99663\n","Consonant Recall(train): 0.99604\n","Score(train): 0.99514\n","---saving model of epoch 46---\n","save finished\n","Epoch number 47\n","Root Recall(train): 0.99460\n","Vowel Recall(train): 0.99650\n","Consonant Recall(train): 0.99476\n","Score(train): 0.99512\n","---saving model of epoch 47---\n","save finished\n","Epoch number 48\n","Root Recall(train): 0.99428\n","Vowel Recall(train): 0.99705\n","Consonant Recall(train): 0.99561\n","Score(train): 0.99531\n","---saving model of epoch 48---\n","save finished\n","Epoch number 49\n","Root Recall(train): 0.99489\n","Vowel Recall(train): 0.99677\n","Consonant Recall(train): 0.99663\n","Score(train): 0.99580\n","---saving model of epoch 49---\n","save finished\n","Epoch number 50\n","Root Recall(train): 0.99482\n","Vowel Recall(train): 0.99716\n","Consonant Recall(train): 0.99525\n","Score(train): 0.99551\n","---saving model of epoch 50---\n","save finished\n","Epoch number 51\n","Root Recall(train): 0.99509\n","Vowel Recall(train): 0.99674\n","Consonant Recall(train): 0.99592\n","Score(train): 0.99571\n","---saving model of epoch 51---\n","save finished\n","Epoch number 52\n","Root Recall(train): 0.99483\n","Vowel Recall(train): 0.99680\n","Consonant Recall(train): 0.99666\n","Score(train): 0.99578\n","---saving model of epoch 52---\n","save finished\n","Epoch number 53\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"890f6d64-deac-4611-956f-0a99ebded2a5","executionInfo":{"status":"ok","timestamp":1583735471124,"user_tz":-540,"elapsed":1294535,"user":{"displayName":"Daisuke Nakajima","photoUrl":"","userId":"10532649301620693567"}},"id":"wMIW8MM2_yfF","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["## 提出ファイルの作成\n","\n","target=[]\n","row_id=[] # row_id place holder\n","\n","for parq_i in range(4):\n","    df_test_img = pd.read_parquet(dataset_dir + f'/test_image_data_{parq_i}.parquet')\n","    # df_test_img = pd.read_parquet(dataset_dir + f'/train_image_data_{parq_i}.parquet') # Error Check!\n","    df_test_img.set_index('image_id', inplace=True)\n","\n","    X_test_resized = resize(df_test_img, out_height=HEIGHT, out_width=WIDTH).astype(np.uint8)\n","\n","    for k, id in enumerate(df_test_img.index.values):\n","        X = X_test_resized[k]\n","\n","        if enable_3d:\n","            X = cv2.resize(X.reshape(HEIGHT, WIDTH), (224, 224),interpolation=cv2.INTER_AREA)\n","            X = X.reshape(224, 224, 1)\n","            X = cv2.cvtColor(X, cv2.COLOR_GRAY2RGB)\n","            X = np.transpose(X, (2,0,1)) / 255.0\n","            X = X.reshape(1, 3, 224, 224) \n","        \n","        else:\n","            X = X.reshape(1, 1, HEIGHT, WIDTH) / 255.0\n","\n","        test_input = torch.tensor(X, dtype=torch.float)\n","        test_input = Variable(test_input)\n","        test_input = try_gpu(test_input)\n","        \n","        model.eval()\n","        root_o, vowel_o, consonant_o = model(test_input)\n","        root_pred, vowel_pred, consonant_pred = torch.max(root_o.data,1)[1], torch.max(vowel_o.data,1)[1], torch.max(consonant_o.data,1)[1]\n","        if torch.cuda.is_available():\n","            root_pred, vowel_pred, consonant_pred = root_pred.to(torch.device(\"cpu\")), vowel_pred.to(torch.device(\"cpu\")), consonant_pred.to(torch.device(\"cpu\"))\n","\n","        root_pred, vowel_pred, consonant_pred = root_pred.item(), vowel_pred.item(), consonant_pred.item()\n","        \n","        row_id.append(id+'_consonant_diacritic')\n","        target.append(consonant_pred)\n","        row_id.append(id+'_grapheme_root')\n","        target.append(root_pred)\n","        row_id.append(id+'_vowel_diacritic')\n","        target.append(vowel_pred)\n","    \n","    del df_test_img\n","    del X_test_resized\n","    gc.collect()\n","\n","\n","df_sample = pd.DataFrame(\n","    {\n","        'row_id': row_id,\n","        'target':target\n","    },\n","    columns = ['row_id','target'] \n",")\n","\n","if create_submission:\n","    df_sample.to_csv('submission.csv',index=False)\n","\n","df_sample.head(36)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Resizing raw image... / 前処理実行中…\n","Resizing raw image... / 前処理実行中…\n","Resizing raw image... / 前処理実行中…\n","Resizing raw image... / 前処理実行中…\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>row_id</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Test_0_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Test_0_grapheme_root</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Test_0_vowel_diacritic</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Test_1_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Test_1_grapheme_root</td>\n","      <td>93</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Test_1_vowel_diacritic</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Test_2_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Test_2_grapheme_root</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Test_2_vowel_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Test_3_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Test_3_grapheme_root</td>\n","      <td>115</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Test_3_vowel_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Test_4_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Test_4_grapheme_root</td>\n","      <td>55</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Test_4_vowel_diacritic</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Test_5_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Test_5_grapheme_root</td>\n","      <td>115</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Test_5_vowel_diacritic</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Test_6_consonant_diacritic</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Test_6_grapheme_root</td>\n","      <td>150</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>Test_6_vowel_diacritic</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>Test_7_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>Test_7_grapheme_root</td>\n","      <td>137</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>Test_7_vowel_diacritic</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>Test_8_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>Test_8_grapheme_root</td>\n","      <td>119</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>Test_8_vowel_diacritic</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>Test_9_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>Test_9_grapheme_root</td>\n","      <td>133</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>Test_9_vowel_diacritic</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>Test_10_consonant_diacritic</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>Test_10_grapheme_root</td>\n","      <td>153</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>Test_10_vowel_diacritic</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>Test_11_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>Test_11_grapheme_root</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>Test_11_vowel_diacritic</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                         row_id  target\n","0    Test_0_consonant_diacritic       0\n","1          Test_0_grapheme_root       3\n","2        Test_0_vowel_diacritic       1\n","3    Test_1_consonant_diacritic       0\n","4          Test_1_grapheme_root      93\n","5        Test_1_vowel_diacritic       2\n","6    Test_2_consonant_diacritic       0\n","7          Test_2_grapheme_root      19\n","8        Test_2_vowel_diacritic       0\n","9    Test_3_consonant_diacritic       0\n","10         Test_3_grapheme_root     115\n","11       Test_3_vowel_diacritic       0\n","12   Test_4_consonant_diacritic       0\n","13         Test_4_grapheme_root      55\n","14       Test_4_vowel_diacritic       4\n","15   Test_5_consonant_diacritic       0\n","16         Test_5_grapheme_root     115\n","17       Test_5_vowel_diacritic       2\n","18   Test_6_consonant_diacritic       5\n","19         Test_6_grapheme_root     150\n","20       Test_6_vowel_diacritic       9\n","21   Test_7_consonant_diacritic       0\n","22         Test_7_grapheme_root     137\n","23       Test_7_vowel_diacritic       7\n","24   Test_8_consonant_diacritic       0\n","25         Test_8_grapheme_root     119\n","26       Test_8_vowel_diacritic       9\n","27   Test_9_consonant_diacritic       0\n","28         Test_9_grapheme_root     133\n","29       Test_9_vowel_diacritic      10\n","30  Test_10_consonant_diacritic       4\n","31        Test_10_grapheme_root     153\n","32      Test_10_vowel_diacritic       1\n","33  Test_11_consonant_diacritic       0\n","34        Test_11_grapheme_root      21\n","35      Test_11_vowel_diacritic       2"]},"metadata":{"tags":[]},"execution_count":11}]}]}
=======
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"submission.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vGJGsJfsggSr","colab_type":"code","outputId":"22e97ddd-a235-4322-cbef-36d0985efd21","executionInfo":{"status":"ok","timestamp":1583761154867,"user_tz":-540,"elapsed":32030,"user":{"displayName":"Daisuke Nakajima","photoUrl":"","userId":"10532649301620693567"}},"colab":{"base_uri":"https://localhost:8080/","height":142}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","%cd gdrive/My\\ Drive/KKB-kaggle/bengaliai-cv19/notebooks"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive/\n","/content/gdrive/My Drive/KKB-kaggle/bengaliai-cv19/notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U67CYVMTg7tE","colab_type":"code","colab":{}},"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# Any results you write to the current directory are saved as output."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yNVEBroeewc4","colab_type":"code","outputId":"eb45d821-822c-404e-a2df-6358b1abd9cb","executionInfo":{"status":"ok","timestamp":1583734183130,"user_tz":-540,"elapsed":6729,"user":{"displayName":"Daisuke Nakajima","photoUrl":"","userId":"10532649301620693567"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["!pip install efficientnet_pytorch"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.6/dist-packages (0.6.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.4.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TqMWjaRU2KU4","colab_type":"code","colab":{}},"source":["## 諸々の import\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import recall_score\n","import cv2\n","# from tqdm.auto import tqdm\n","import copy\n","import datetime\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms, utils\n","import torchvision.models as models\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import torchvision\n","import PIL\n","# from torchsummary import summary\n","import gc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RbsZSbkg9SrB","colab_type":"code","colab":{}},"source":["## Parameters\n","\n","# resize後のサイズ\n","HEIGHT = 64\n","WIDTH = 64\n","\n","# 画像を3次元にするかどうか（EfficientNetなどを使うときはTrue）\n","enable_3d = True\n","\n","# True なら Cross Validation を実施する\n","# Kaggle に提出するデータを作るときは False にしてください\n","do_validation = False\n","\n","# True なら submission.csv を生成する\n","create_submission = False\n","\n","val_perc = 0.2  # validation set の割合（クロスバリデーション）\n","\n","#epochs number\n","epochs = 30\n","\n","#初期値として、保存した重みを使う時はこれをTrueに\n","load_flag = True\n","\n","#Kaggleで提出するときはTrueにする\n","kaggle_flag = False\n","\n","#loadするファイルへのパス\n","if kaggle_flag:\n","    model_path = '/kaggle/input/model_load/resnet18_epoch1_2020-03-09_14-58-43.pth'\n","else:\n","    model_path = '../trained_models/resnet18_epoch32_2020-03-09_17-18-32.pth'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f2uk6pfBBxdy","colab_type":"code","colab":{}},"source":["if kaggle_flag:\n","    dataset_dir = '/kaggle/input/bengaliai-cv19'\n","    model_dir = '/kaggle/input/trained_models'\n","else:\n","    dataset_dir = '../dataset'\n","    model_dir = '../trained_models'\n","\n","train_df = pd.read_csv(dataset_dir + '/train.csv')\n","test_df = pd.read_csv(dataset_dir + '/test.csv')\n","class_map_df = pd.read_csv(dataset_dir + '/class_map.csv')\n","sample_sub_df = pd.read_csv(dataset_dir + '/sample_submission.csv')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RU66nFHqgj7K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"bb1bef9e-ab2e-4acd-d9da-8f879294e62b","executionInfo":{"status":"ok","timestamp":1583761272841,"user_tz":-540,"elapsed":4227,"user":{"displayName":"Daisuke Nakajima","photoUrl":"","userId":"10532649301620693567"}}},"source":["#モデルの設定\n","\n","from preprocess import *\n","from save_load import *\n","#from model.CNN import model\n","#from model.efficientnet import model\n","from model.resnet18 import model\n","#from model.resnet34 import model\n","#from model.resnet50 import model\n","#from model.resnet101 import model\n","#from model.resnet152 import model\n","\n","model = model()\n","model = try_gpu(model)\n","optimizer = optim.Adam(model.parameters())\n","if load_flag:\n","    model,optimizer,start_epoch = load_model(model,optimizer,model_path)\n","    model = try_gpu(model)\n","    start_epoch += 1\n","else:\n","    start_epoch = 1    \n","criterion1 = nn.CrossEntropyLoss() "],"execution_count":7,"outputs":[{"output_type":"stream","text":["loaded from ../trained_models/resnet18_epoch32_2020-03-09_17-18-32.pth\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Vket_LbeGIcv","colab_type":"code","colab":{}},"source":["##もし今までのmodelの重みのみを保存しているファイルから読み込む場合\n","device = torch.device('cpu')\n","model.load_state_dict(torch.load(model_dir+\"/resnet18.pth\",map_location=device)) #各自パスを設定\n","model = try_gpu(model)\n","start_epoch = 31 #各自変更"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7G-i0XLtsbY","colab_type":"code","colab":{}},"source":["## train関数、test関数\n","\n","def train(model, epoch, train_loader):\n","    model.train()\n","    \n","    size = len(train_loader.dataset)\n","    pred_r, pred_v, pred_c = np.zeros(size), np.zeros(size), np.zeros(size)\n","    true_r, true_v, true_c = np.zeros(size), np.zeros(size), np.zeros(size)\n","    index = 0\n","\n","    for data in train_loader:\n","        inputs, root_l, vowel_l, consonant_l = data\n","        inputs, root_l, vowel_l, consonant_l = Variable(inputs), Variable(root_l), Variable(vowel_l), Variable(consonant_l)\n","        inputs, root_l, vowel_l, consonant_l = try_gpu(inputs), try_gpu(root_l), try_gpu(vowel_l), try_gpu(consonant_l)\n","        optimizer.zero_grad()\n","        root_o, vowel_o, consonant_o = model(inputs)\n","        root_pred, vowel_pred, consonant_pred = torch.max(root_o.data,1)[1], torch.max(vowel_o.data,1)[1], torch.max(consonant_o.data,1)[1]\n","        loss1 = criterion1(root_o, root_l)\n","        loss2 = criterion1(vowel_o, vowel_l)\n","        loss3 = criterion1(consonant_o, consonant_l)\n","        (loss1+loss2+loss3).backward()\n","        optimizer.step()\n","        for i in range(inputs.size(0)):\n","            pred_r[index] = root_pred[i]\n","            pred_v[index] = vowel_pred[i]\n","            pred_c[index] = consonant_pred[i]\n","            true_r[index] = root_l[i]\n","            true_v[index] = vowel_l[i]\n","            true_c[index] = consonant_l[i]\n","            index += 1\n","    recall_r = recall_score(true_r, pred_r, average='macro')\n","    recall_v = recall_score(true_v, pred_v, average='macro')\n","    recall_c = recall_score(true_c, pred_c, average='macro')\n","    final_score = (2.*recall_r + recall_v + recall_c) / 4.\n","\n","    print(f'Root Recall(train): {recall_r:.5f}')\n","    print(f'Vowel Recall(train): {recall_v:.5f}')\n","    print(f'Consonant Recall(train): {recall_c:.5f}')\n","    print(f'Score(train): {final_score:.5f}')\n","   \n","\n","def test(model, test_loader):\n","    model.eval()\n","\n","    size = len(test_loader.dataset)\n","    pred_r, pred_v, pred_c = np.zeros(size), np.zeros(size), np.zeros(size)\n","    true_r, true_v, true_c = np.zeros(size), np.zeros(size), np.zeros(size)\n","    index = 0\n","    \n","    for data in test_loader:\n","        inputs, root_l, vowel_l, consonant_l = data\n","        inputs, root_l, vowel_l, consonant_l = Variable(inputs), Variable(root_l), Variable(vowel_l), Variable(consonant_l)\n","        inputs, root_l, vowel_l, consonant_l = try_gpu(inputs), try_gpu(root_l), try_gpu(vowel_l), try_gpu(consonant_l)\n","        \n","        root_o, vowel_o, consonant_o = model(inputs) \n","        root_pred, vowel_pred, consonant_pred = torch.max(root_o.data,1)[1], torch.max(vowel_o.data,1)[1], torch.max(consonant_o.data,1)[1]\n","        for i in range(inputs.size(0)):\n","            pred_r[index] = root_pred[i]\n","            pred_v[index] = vowel_pred[i]\n","            pred_c[index] = consonant_pred[i]\n","            true_r[index] = root_l[i]\n","            true_v[index] = vowel_l[i]\n","            true_c[index] = consonant_l[i]\n","            index += 1\n","\n","    recall_r = recall_score(true_r, pred_r, average='macro')\n","    recall_v = recall_score(true_v, pred_v, average='macro')\n","    recall_c = recall_score(true_c, pred_c, average='macro')\n","    final_score = (2.*recall_r + recall_v + recall_c) / 4.\n","\n","    print(f'Root Recall(test): {recall_r:.5f}')\n","    print(f'Vowel Recall(test): {recall_v:.5f}')\n","    print(f'Consonant Recall(test): {recall_c:.5f}')\n","    print(f'Score(test): {final_score:.5f}')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5wZ0lQFJ3Ajz","colab_type":"code","outputId":"3a41124d-a4cd-44dc-bc45-407e9b279182","executionInfo":{"status":"ok","timestamp":1583737594253,"user_tz":-540,"elapsed":375544,"user":{"displayName":"Daisuke Nakajima","photoUrl":"","userId":"10532649301620693567"}},"colab":{"base_uri":"https://localhost:8080/","height":157}},"source":["## データの読み込み\n","\n","X_all = np.empty((0, HEIGHT*WIDTH))\n","Y_root_all = np.empty((0, 168))\n","Y_vowel_all = np.empty((0, 11))\n","Y_cons_all = np.empty((0, 7))\n","\n","for parq_i in range(4):\n","    print(f'Parquet {parq_i} を読み込み中')\n","    train_df_with_img = pd.merge(pd.read_parquet(dataset_dir + f'/train_image_data_{parq_i}.parquet'), train_df, on='image_id').drop(['image_id'], axis=1)\n","    \n","    X = train_df_with_img.drop(columns=['grapheme_root', 'vowel_diacritic', 'consonant_diacritic', 'grapheme'])\n","    X_resized = resize(X, out_height=HEIGHT, out_width=WIDTH).astype(np.uint8) # astype(np.uint8)をしてあげることで後で cv2.cvtColor(out_data, cv2.COLOR_GRAY2RGB) が実行できるようになる\n","    \n","    Y_root = pd.get_dummies(train_df_with_img['grapheme_root']).values\n","    Y_vowel = pd.get_dummies(train_df_with_img['vowel_diacritic']).values\n","    Y_cons = pd.get_dummies(train_df_with_img['consonant_diacritic']).values\n","\n","    X_all = np.append(X_all, X_resized, axis=0)\n","    Y_root_all = np.append(Y_root_all, Y_root, axis=0)\n","    Y_vowel_all = np.append(Y_vowel_all, Y_vowel, axis=0)\n","    Y_cons_all = np.append(Y_cons_all, Y_cons, axis=0)\n","\n","    del X\n","    del X_resized\n","    del Y_root\n","    del Y_vowel \n","    del Y_cons \n","    gc.collect()\n","\n","print(X_all.shape)\n","print(Y_root_all.shape)\n","print(Y_vowel_all.shape)\n","print(Y_cons_all.shape)\n","\n","Y_all = [Y_root_all, Y_vowel_all, Y_cons_all]\n","\n","trainval_dataset = MyDataset(X_all, Y_all, enable_3d=enable_3d, H=HEIGHT, W=WIDTH)\n","\n","del X_all\n","del Y_root_all\n","del Y_vowel_all\n","del Y_cons_all\n","del Y_all\n","gc.collect()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Parquet 0 を読み込み中\n","Resizing raw image... / 前処理実行中…\n","Parquet 1 を読み込み中\n","Resizing raw image... / 前処理実行中…\n","Parquet 2 を読み込み中\n","Resizing raw image... / 前処理実行中…\n","Parquet 3 を読み込み中\n","Resizing raw image... / 前処理実行中…\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ee2x8byvB8eW","colab_type":"code","outputId":"07def2db-ad08-4499-8756-9905d73481a7","executionInfo":{"status":"ok","timestamp":1583741913922,"user_tz":-540,"elapsed":3299607,"user":{"displayName":"Daisuke Nakajima","photoUrl":"","userId":"10532649301620693567"}},"colab":{"base_uri":"https://localhost:8080/","height":455}},"source":["## 訓練ループ / Training Loop Kaggleでやる際には使わない\n","\n","print('==========================')\n","print('Starting training.')\n","\n","if do_validation:\n","    n_samples = len(trainval_dataset)\n","    train_size = int(len(trainval_dataset)*(1.0 - val_perc))\n","    val_size = n_samples - train_size\n","    print(f'train size: {train_size}, validation size: {val_size}')\n","\n","    subset, val_dataset = torch.utils.data.random_split(trainval_dataset, [train_size, val_size])\n","\n","    if enable_3d:\n","        train_dataset = TransformDataset(subset, transform=transforms.RandomChoice(\n","            [transform_none, transform_crop224, transform_rotate, transform_noise]\n","        ))\n","\n","    else:\n","        train_dataset = TransformDataset(subset, transform=transforms.RandomChoice(\n","            [transform_none, transform_crop64, transform_rotate, transform_noise]\n","        ))\n","\n","    del trainval_dataset\n","    del subset\n","    gc.collect()\n","\n","    train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=4)\n","    val_loader = DataLoader(dataset=val_dataset, batch_size=32, num_workers=0)\n","\n","    for i in range(start_epoch,epochs+start_epoch):\n","        print(f'Epoch number {i}')\n","        train(model, i, train_loader)\n","        test(model, val_loader)\n","        save_model(model,optimizer,model_dir, i)\n","\n","    # メモリ節約\n","    del train_dataset\n","    del val_dataset\n","    del train_loader\n","    del val_loader\n","    gc.collect()\n","\n","else:\n","    n_samples = len(trainval_dataset)\n","    print(f'train size: {n_samples}')\n","\n","    if enable_3d:\n","        trainval_dataset.transform = transforms.RandomChoice(\n","            [transform_none, transform_crop224, transform_rotate, transform_noise]\n","        )\n","    \n","    else:\n","        trainval_dataset.transform = transforms.RandomChoice(\n","            [transform_none, transform_crop64, transform_rotate, transform_noise]\n","        )\n","\n","    train_loader = DataLoader(dataset=trainval_dataset, batch_size=32, shuffle=True, num_workers=4)\n","\n","    for i in range(start_epoch,epochs+start_epoch):\n","        print(f'Epoch number {i}')\n","        train(model, i, train_loader)\n","        save_model(model,optimizer,model_dir, i)\n","\n","    # メモリ節約\n","    del trainval_dataset\n","    del train_loader\n","    gc.collect()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["==========================\n","Starting training.\n","train size: 160672, validation size: 40168\n","Epoch number 31\n","Root Recall(train): 0.96560\n","Vowel Recall(train): 0.98601\n","Consonant Recall(train): 0.98270\n","Score(train): 0.97498\n","Root Recall(test): 0.97411\n","Vowel Recall(test): 0.98884\n","Consonant Recall(test): 0.99374\n","Score(test): 0.98270\n","---saving model of epoch 31---\n","save finished\n","Epoch number 32\n","Root Recall(train): 0.97135\n","Vowel Recall(train): 0.98702\n","Consonant Recall(train): 0.98507\n","Score(train): 0.97870\n","Root Recall(test): 0.97794\n","Vowel Recall(test): 0.99361\n","Consonant Recall(test): 0.98701\n","Score(test): 0.98412\n","---saving model of epoch 32---\n","save finished\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"890f6d64-deac-4611-956f-0a99ebded2a5","executionInfo":{"status":"ok","timestamp":1583735471124,"user_tz":-540,"elapsed":1294535,"user":{"displayName":"Daisuke Nakajima","photoUrl":"","userId":"10532649301620693567"}},"id":"wMIW8MM2_yfF","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["## 提出ファイルの作成\n","\n","target=[]\n","row_id=[] # row_id place holder\n","\n","for parq_i in range(4):\n","    df_test_img = pd.read_parquet(dataset_dir + f'/test_image_data_{parq_i}.parquet')\n","    # df_test_img = pd.read_parquet(dataset_dir + f'/train_image_data_{parq_i}.parquet') # Error Check!\n","    df_test_img.set_index('image_id', inplace=True)\n","\n","    X_test_resized = resize(df_test_img, out_height=HEIGHT, out_width=WIDTH).astype(np.uint8)\n","\n","    for k, id in enumerate(df_test_img.index.values):\n","        X = X_test_resized[k]\n","\n","        if enable_3d:\n","            X = cv2.resize(X.reshape(HEIGHT, WIDTH), (224, 224),interpolation=cv2.INTER_AREA)\n","            X = X.reshape(224, 224, 1)\n","            X = cv2.cvtColor(X, cv2.COLOR_GRAY2RGB)\n","            X = np.transpose(X, (2,0,1)) / 255.0\n","            X = X.reshape(1, 3, 224, 224) \n","        \n","        else:\n","            X = X.reshape(1, 1, HEIGHT, WIDTH) / 255.0\n","\n","        test_input = torch.tensor(X, dtype=torch.float)\n","        test_input = Variable(test_input)\n","        test_input = try_gpu(test_input)\n","        \n","        model.eval()\n","        root_o, vowel_o, consonant_o = model(test_input)\n","        root_pred, vowel_pred, consonant_pred = torch.max(root_o.data,1)[1], torch.max(vowel_o.data,1)[1], torch.max(consonant_o.data,1)[1]\n","        if torch.cuda.is_available():\n","            root_pred, vowel_pred, consonant_pred = root_pred.to(torch.device(\"cpu\")), vowel_pred.to(torch.device(\"cpu\")), consonant_pred.to(torch.device(\"cpu\"))\n","\n","        root_pred, vowel_pred, consonant_pred = root_pred.item(), vowel_pred.item(), consonant_pred.item()\n","        \n","        row_id.append(id+'_consonant_diacritic')\n","        target.append(consonant_pred)\n","        row_id.append(id+'_grapheme_root')\n","        target.append(root_pred)\n","        row_id.append(id+'_vowel_diacritic')\n","        target.append(vowel_pred)\n","    \n","    del df_test_img\n","    del X_test_resized\n","    gc.collect()\n","\n","\n","df_sample = pd.DataFrame(\n","    {\n","        'row_id': row_id,\n","        'target':target\n","    },\n","    columns = ['row_id','target'] \n",")\n","\n","if create_submission:\n","    df_sample.to_csv('submission.csv',index=False)\n","\n","df_sample.head(36)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Resizing raw image... / 前処理実行中…\n","Resizing raw image... / 前処理実行中…\n","Resizing raw image... / 前処理実行中…\n","Resizing raw image... / 前処理実行中…\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>row_id</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Test_0_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Test_0_grapheme_root</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Test_0_vowel_diacritic</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Test_1_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Test_1_grapheme_root</td>\n","      <td>93</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Test_1_vowel_diacritic</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Test_2_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Test_2_grapheme_root</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Test_2_vowel_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Test_3_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Test_3_grapheme_root</td>\n","      <td>115</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Test_3_vowel_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Test_4_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Test_4_grapheme_root</td>\n","      <td>55</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Test_4_vowel_diacritic</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Test_5_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Test_5_grapheme_root</td>\n","      <td>115</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Test_5_vowel_diacritic</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Test_6_consonant_diacritic</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Test_6_grapheme_root</td>\n","      <td>150</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>Test_6_vowel_diacritic</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>Test_7_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>Test_7_grapheme_root</td>\n","      <td>137</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>Test_7_vowel_diacritic</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>Test_8_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>Test_8_grapheme_root</td>\n","      <td>119</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>Test_8_vowel_diacritic</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>Test_9_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>Test_9_grapheme_root</td>\n","      <td>133</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>Test_9_vowel_diacritic</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>Test_10_consonant_diacritic</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>Test_10_grapheme_root</td>\n","      <td>153</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>Test_10_vowel_diacritic</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>Test_11_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>Test_11_grapheme_root</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>Test_11_vowel_diacritic</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                         row_id  target\n","0    Test_0_consonant_diacritic       0\n","1          Test_0_grapheme_root       3\n","2        Test_0_vowel_diacritic       1\n","3    Test_1_consonant_diacritic       0\n","4          Test_1_grapheme_root      93\n","5        Test_1_vowel_diacritic       2\n","6    Test_2_consonant_diacritic       0\n","7          Test_2_grapheme_root      19\n","8        Test_2_vowel_diacritic       0\n","9    Test_3_consonant_diacritic       0\n","10         Test_3_grapheme_root     115\n","11       Test_3_vowel_diacritic       0\n","12   Test_4_consonant_diacritic       0\n","13         Test_4_grapheme_root      55\n","14       Test_4_vowel_diacritic       4\n","15   Test_5_consonant_diacritic       0\n","16         Test_5_grapheme_root     115\n","17       Test_5_vowel_diacritic       2\n","18   Test_6_consonant_diacritic       5\n","19         Test_6_grapheme_root     150\n","20       Test_6_vowel_diacritic       9\n","21   Test_7_consonant_diacritic       0\n","22         Test_7_grapheme_root     137\n","23       Test_7_vowel_diacritic       7\n","24   Test_8_consonant_diacritic       0\n","25         Test_8_grapheme_root     119\n","26       Test_8_vowel_diacritic       9\n","27   Test_9_consonant_diacritic       0\n","28         Test_9_grapheme_root     133\n","29       Test_9_vowel_diacritic      10\n","30  Test_10_consonant_diacritic       4\n","31        Test_10_grapheme_root     153\n","32      Test_10_vowel_diacritic       1\n","33  Test_11_consonant_diacritic       0\n","34        Test_11_grapheme_root      21\n","35      Test_11_vowel_diacritic       2"]},"metadata":{"tags":[]},"execution_count":11}]}]}
>>>>>>> 3e8e4fb6607de4021733cc21e6a50b02ace01fa0
